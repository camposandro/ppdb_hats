{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c6b1cb4",
   "metadata": {},
   "source": [
    "# PPDB in HATS: demo and validation\n",
    "\n",
    "In this notebook we will:\n",
    "\n",
    "1. Import the pre-existing HATS catalog of PPDB.\n",
    "2. Increment the catalog with data for 5 days.\n",
    "3. Perform the full weekly reimport.\n",
    "4. Validate they have the same objects and sources.\n",
    "5. Compare speed of access between the incremented vs full reimport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04832e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lsdb\n",
    "import nested_pandas as npd\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import date\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "PPDB_HATS_DIR = Path(\"/sdf/data/rubin/shared/lsdb_commissioning/ppdb\")\n",
    "PPDB_LSST_DIR = Path(\"/sdf/scratch/rubin/ppdb/data/ppdb_lsstcam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235851f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls {PPDB_LSST_DIR}/2026/01/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d719c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_files_per_date(date):\n",
    "    obj_files = glob(f\"{PPDB_LSST_DIR}/{date}/**/DiaObject*.parquet\")\n",
    "    src_files = glob(f\"{PPDB_LSST_DIR}/{date}/**/DiaSource*.parquet\")\n",
    "    fsrc_files = glob(f\"{PPDB_LSST_DIR}/{date}/**/DiaForcedSource*.parquet\")\n",
    "    print(f\"Number of DiaObject files: {len(obj_files)}\")\n",
    "    print(f\"Number of DiaSource files: {len(src_files)}\")\n",
    "    print(f\"Number of DiaForcedSource files: {len(fsrc_files)}\")\n",
    "    return obj_files, src_files, fsrc_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04fb84f",
   "metadata": {},
   "source": [
    "### Import pre-existing catalog\n",
    "\n",
    "The pre-existing catalog has data from Sept 2025 to Jan 20, 2026."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1bec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_dir = PPDB_HATS_DIR / \"dia_object_collection\"\n",
    "collection = lsdb.open_catalog(collection_dir)\n",
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa4fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.plot_coverage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93544f9",
   "metadata": {},
   "source": [
    "All the input parquet paths are stored under `input_paths`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394fa9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -10 {collection_dir}/input_paths/dia_object.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3093e813",
   "metadata": {},
   "source": [
    "### Daily increment\n",
    "\n",
    "We will increment with data from a couple of days (Jan 24 and Jan 25), one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c119cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_files, src_files, fsrc_files = _find_files_per_date(\"2026/01/24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966357dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ppdb_hats.config import get_default_config\n",
    "\n",
    "config = get_default_config(until_date=date(2026, 1, 24))\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729760b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ppdb_hats import DailyPipeline\n",
    "\n",
    "DailyPipeline(config=config).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930d7c65",
   "metadata": {},
   "source": [
    "#### Validation of first increment\n",
    "\n",
    "Let's validate that the data looks consistent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76245df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -10 {collection_dir}/input_paths/dia_object.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44b279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "increment = lsdb.open_catalog(collection_dir)\n",
    "print(f\"The collection has {len(increment):,} objects.\")\n",
    "print(f\"The increment added {len(increment) - len(collection):,} new objects.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b76c73b",
   "metadata": {},
   "source": [
    "This is the expected number of objects, after de-duplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341a74ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_objects = npd.read_parquet(obj_files)\n",
    "unique_ids = new_objects[\"diaObjectId\"].unique()\n",
    "increment_ids = increment[\"diaObjectId\"].compute()\n",
    "assert set(unique_ids).issubset(set(increment_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca91296e",
   "metadata": {},
   "source": [
    "Looking at a specific object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a0936",
   "metadata": {},
   "outputs": [],
   "source": [
    "increment.query(f\"diaObjectId == {unique_ids[1]}\").compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa80d09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_objects.query(f\"diaObjectId == {unique_ids[1]}\").sort_values(\"validityStartMjdTai\").tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a398633",
   "metadata": {},
   "source": [
    "Making sure this object's new sources were appended correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3481ea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sources = npd.read_parquet(src_files)\n",
    "obj_new_sources = new_sources.query(f\"diaObjectId == {unique_ids[1]}\")\n",
    "obj_new_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c340beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "increment_obj = increment.query(f\"diaObjectId == {unique_ids[1]}\")\n",
    "obj_sources = increment_obj[\"diaSource\"].compute().explode()\n",
    "pd.concat([obj_sources.query(f\"diaSourceId == {id}\") for id in obj_new_sources[\"diaSourceId\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794e05dd",
   "metadata": {},
   "source": [
    "We keep track of the already imported parquet paths. On an increment, those are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee109420",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_files, src_files, fsrc_files = _find_files_per_date(\"2026/01/25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4191668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_default_config(until_date=date(2026, 1, 25))\n",
    "DailyPipeline(config=config).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8327c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -10 {collection_dir}/input_paths/dia_object.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e51cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "increment2 = lsdb.open_catalog(collection_dir)\n",
    "print(f\"The collection has {len(increment2):,} objects.\")\n",
    "print(f\"The increment added {len(increment2) - len(increment):,} new objects.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c63ef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_objects = npd.read_parquet(obj_files)\n",
    "unique_ids = new_objects[\"diaObjectId\"].unique()\n",
    "increment2_ids = increment2[\"diaObjectId\"].compute()\n",
    "assert set(unique_ids).issubset(set(increment2_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84993da7",
   "metadata": {},
   "source": [
    "Let's check the object we were working on previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047aa094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We grabbed the most recent object data from the input files\n",
    "new_objects.query(f\"diaObjectId == 169747015140900902\").sort_values(\n",
    "    \"validityStartMjdTai\", ascending=False\n",
    ").tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52580d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's present in the updated HATS collection:\n",
    "increment2.query(f\"diaObjectId == 169747015140900902\").compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6158a0e",
   "metadata": {},
   "source": [
    "We now have a parquet file for each import, which we can query (even with other parquet readers) to get specific date updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2350fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls {collection_dir}/dia_object_lc/dataset/Norder=2/Dir=0/Npix=106"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd68eef3",
   "metadata": {},
   "source": [
    "### Weekly reimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41461cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e684891c",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c777c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that the reimported catalog equals to the"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
